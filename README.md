# Bird Embeddings with Variational Autoencoders

A PyTorch implementation of Variational Autoencoders (VAEs) for learning compressed embeddings from eBird checklist data. This project transforms high-dimensional bird species presence-absence data into meaningful low-dimensional representations that capture ecological patterns.

## ğŸ¯ Project Overview

**What it does**: Converts eBird checklists (e.g., 500+ species) into compact embeddings (e.g., 16 dimensions) that preserve ecological information.

**Why it's useful**:
- **Dimensionality reduction**: Compress large species lists into manageable vectors
- **Similarity search**: Find similar checklists based on species composition
- **Visualization**: Plot checklists in 2D/3D space
- **Feature extraction**: Use embeddings for downstream ML tasks (classification, clustering, etc.)

**Important Note**: This repository contains **code and workflows only**. Large data files and trained models are excluded due to GitHub size limits. Follow the [Getting Started](#-getting-started) section to generate them.

## ğŸ“ Project Structure

```
bird-embeddings/
â”œâ”€â”€ src/                        # âœ… Core modules (reusable code)
â”‚   â”œâ”€â”€ data/                   # Data loading and preprocessing
â”‚   â”‚   â”œâ”€â”€ loader.py          # Load eBird TSV files
â”‚   â”‚   â”œâ”€â”€ preprocessor.py    # Create species matrices
â”‚   â”‚   â”œâ”€â”€ dataset.py         # PyTorch Dataset classes
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ models/                 # VAE model architecture
â”‚   â”‚   â”œâ”€â”€ vae.py             # VariationalAutoencoder class
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ training/               # Training utilities
â”‚   â”‚   â”œâ”€â”€ trainer.py         # Training loop with noise augmentation
â”‚   â”‚   â”œâ”€â”€ utils.py           # Checkpointing, device management
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ inference/              # Embedding extraction
â”‚       â”œâ”€â”€ extractor.py       # EmbeddingExtractor class
â”‚       â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/                       # âš ï¸ NOT INCLUDED (too large)
â”‚   â”œâ”€â”€ raw/                    # Place eBird data here
â”‚   â”‚   â””â”€â”€ ebd_IN-KL_smp_relSep-2025.txt  # Download from eBird
â”‚   â””â”€â”€ processed/              # Generated embeddings (created by you)
â”‚       â””â”€â”€ kerala_embeddings.npz
â”‚
â”œâ”€â”€ models/                     # âš ï¸ NOT INCLUDED (too large)
â”‚   â””â”€â”€ vae_kerala.pth         # Trained models (created by you)
â”‚
â”œâ”€â”€ checkpoints/                # Training checkpoints (created during training)
â”‚
â”œâ”€â”€ scripts/                    # âœ… Utility scripts
â”‚   â”œâ”€â”€ train_new_model.py     # Train and save compatible models
â”‚   â”œâ”€â”€ check_model_compatibility.py  # Verify model format
â”‚   â””â”€â”€ test_extractor_fix.py  # Quick inference test
â”‚
â”œâ”€â”€ notebooks/                  # âœ… Training notebooks
â”‚   â””â”€â”€ train_vae_kerala.ipynb # Example training workflow
â”‚
â”œâ”€â”€ test_notebooks/             # âœ… Test suite
â”‚   â”œâ”€â”€ test_vae_module.ipynb
â”‚   â”œâ”€â”€ test_data_pipeline.ipynb
â”‚   â”œâ”€â”€ test_training.ipynb
â”‚   â””â”€â”€ test_inference.ipynb
â”‚
â”œâ”€â”€ projects/                   # âœ… Example downstream tasks
â”‚   â”œâ”€â”€ district_prediction/   # Predict district from embeddings
â”‚   â”œâ”€â”€ wetland_prediction/    # Classify wetland habitats
â”‚   â””â”€â”€ _template/             # Project template for new analyses
â”‚
â”œâ”€â”€ docs/                       # âœ… Documentation
â”‚   â”œâ”€â”€ INFERENCE_FIX_SUMMARY.md
â”‚   â”œâ”€â”€ TEST_INFERENCE_READY.md
â”‚   â”œâ”€â”€ ORGANIZATION_SUMMARY.md
â”‚   â””â”€â”€ PHASE_2_SUMMARY.md
â”‚
â”œâ”€â”€ requirements.txt            # âœ… Python dependencies
â”œâ”€â”€ .gitignore                  # âœ… Ignore large files and old notebooks
â””â”€â”€ README.md                   # This file
```

**Legend:**
- âœ… Included in repository
- âš ï¸ Not included (generate yourself following the workflow)


## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- PyTorch
- Pandas, NumPy, scikit-learn
- Jupyter Notebook (for interactive analysis)

### Installation

```bash
# Clone repository
git clone <repository-url>
cd bird-embeddings

# Install dependencies
pip install -r requirements.txt
```

### Data Setup

**Required:** Download Kerala eBird data from [eBird](https://ebird.org/data/download)

1. Request eBird Basic Dataset (EBD) for **Kerala, India** (region code: `IN-KL`)
2. Download the sampling event file: `ebd_IN-KL_smp_relSep-2025.txt` (or latest)
3. Place the file in `data/raw/`:

```bash
data/
â””â”€â”€ raw/
    â””â”€â”€ ebd_IN-KL_smp_relSep-2025.txt  # Your eBird data file
```

**Note:** The actual data and trained models are NOT included in this repository due to size constraints. You need to generate them following the workflow below.

---

## ğŸ“‹ Complete Workflow

### Step 1: Prepare Your Data

Load and explore the eBird data:

```python
from src.data import load_ebird_data, get_ebird_columns

# Check what columns are available
columns = get_ebird_columns("ebd_IN-KL_smp_relSep-2025.txt")
print(f"Dataset has {len(columns)} columns")

# Load full dataset (or use nrows=100000 for testing)
data = load_ebird_data("ebd_IN-KL_smp_relSep-2025.txt")
print(f"Loaded {len(data)} observations")
```

### Step 2: Create Species Matrix

Convert observation-level data to checklist-level species matrix:

```python
from src.data import create_species_matrix

# Create binary species presence-absence matrix
matrix, species_list = create_species_matrix(
    data,
    min_species_observations=30,  # Filter rare species
    min_checklist_species=5,      # Filter sparse checklists
    apply_quality_filters=True    # Use complete checklists only
)

print(f"Matrix shape: {matrix.shape[0]} checklists Ã— {len(species_list)} species")
```

### Step 3: Train the VAE Model

Train a Variational Autoencoder to learn embeddings:

```python
from src.models import VariationalAutoencoder
from src.data import split_train_val, create_dataloaders
from src.training import train_variational_autoencoder, get_device, set_seed

# Set random seed for reproducibility
set_seed(42)

# Split data into train/validation
train_ds, val_ds = split_train_val(matrix, species_list, val_size=0.2)

# Create DataLoaders
train_loader, val_loader = create_dataloaders(
    train_ds, val_ds,
    batch_size=128,
    num_workers=4
)

# Create model
model = VariationalAutoencoder(
    input_dimension=len(species_list),
    latent_dimension=16,  # Embedding size
    hidden_dimension=512
)

# Train the model
device = get_device()  # Auto-detect CUDA/MPS/CPU
history = train_variational_autoencoder(
    model,
    train_loader,
    val_loader,
    num_epochs=50,
    learning_rate=1e-3,
    noise_std=0.1,
    device=device,
    checkpoint_dir='checkpoints/my_vae',
    save_every=10
)
```

**Or use the training script:**

```bash
python scripts/train_new_model.py
```

### Step 4: Save Model for Inference

Save the trained model in a compatible format:

```python
from src.training import save_model_for_inference

save_model_for_inference(
    model=model,
    filepath='models/vae_kerala.pth',
    input_dim=len(species_list),
    latent_dim=16,
    hidden_dims=[512]
)
```

### Step 5: Extract Embeddings

Use the trained model to extract embeddings from checklists:

```python
from src.inference import EmbeddingExtractor

# Load the trained model
extractor = EmbeddingExtractor('models/vae_kerala.pth', device='cpu')

# Extract embeddings (deterministic mode)
embeddings = extractor.extract_embeddings(matrix, use_mean=True)
print(f"Embeddings shape: {embeddings.shape}")  # [num_checklists, 16]

# Save embeddings for downstream tasks
extractor.save_embeddings(
    embeddings,
    'data/processed/kerala_embeddings.npz',
    checklist_ids=matrix.index.tolist()
)
```

### Step 6: Use Embeddings for Downstream Tasks

See example projects in `projects/`:

- **District Prediction** (`projects/district_prediction/`): Predict Kerala district from species composition
- **Wetland Classification** (`projects/wetland_prediction/`): Identify wetland habitats using embeddings

---

## ğŸ§ª Testing & Validation

### Quick Test

Verify everything is working:

```bash
# Test inference module
python scripts/test_extractor_fix.py

# Test data pipeline
jupyter notebook test_notebooks/test_data_pipeline.ipynb

# Test training
jupyter notebook test_notebooks/test_training.ipynb

# Test inference
jupyter notebook test_notebooks/test_inference.ipynb
```

### Check Model Compatibility

```bash
python scripts/check_model_compatibility.py models/vae_kerala.pth
```

## ğŸ“Š Data Format

**Input**: eBird checklist data in TSV format
- Columns include species observations, location, date, etc.
- Each row represents a single species observation in a checklist

**Preprocessed**: Binary/one-hot encoded species presence-absence matrix
- Rows = checklists
- Columns = species
- Values = 0 (absent) or 1 (present)

**Output**: Continuous embeddings
- Each checklist â†’ 16-dimensional vector (configurable)

## ğŸ§  Model Architecture

```
Input: [batch_size, num_species] (e.g., [32, 500])
    â†“
Encoder: 3-layer MLP (500 â†’ 512 â†’ 512 â†’ 512)
    â†“
Latent Parameters: Î¼ and log(ÏƒÂ²) (each [batch_size, 16])
    â†“
Reparameterization: z = Î¼ + Ïƒ * Îµ, where Îµ ~ N(0,1)
    â†“
Decoder: 3-layer MLP (16 â†’ 512 â†’ 512 â†’ 512 â†’ 500)
    â†“
Output: [batch_size, num_species] (reconstructed, sigmoid)
```

**Loss Function**:
```
Total Loss = BCE(reconstruction, input) + KL(N(Î¼, ÏƒÂ²) || N(0, 1))
```

- **Reconstruction Loss**: Binary cross-entropy between input and reconstruction
- **KL Divergence**: Regularizes latent space to follow standard normal distribution

## ğŸ“ˆ Current Status

### âœ… Complete Modules

**Phase 1-5: Core Infrastructure - COMPLETE**
- âœ… Project structure and organization
- âœ… VAE model architecture (`src/models/`)
- âœ… Data loading and preprocessing (`src/data/`)
- âœ… Training pipeline with noise augmentation (`src/training/`)
- âœ… Inference and embedding extraction (`src/inference/`)
- âœ… All test notebooks passing
- âœ… Utility scripts for training and testing
- âœ… Comprehensive documentation

**Example Projects - COMPLETE**
- âœ… District prediction (Random Forest + Neural Network)
- âœ… Wetland habitat classification (three approaches)

### ğŸ¯ What You Can Do

With this repository, you can:

1. **Train VAE models** on your own eBird data
2. **Extract embeddings** from bird checklists
3. **Use embeddings** for downstream ML tasks:
   - Geographic prediction (district, region)
   - Habitat classification (wetland, forest, etc.)
   - Species distribution modeling
   - Checklist similarity search
   - Clustering analysis

### âš ï¸ What's NOT Included

Due to GitHub file size limits:
- âŒ Raw eBird data files (download from eBird.org)
- âŒ Trained model checkpoints (generate using workflow)
- âŒ Processed embeddings (extract using inference module)
- âŒ Old/legacy notebooks (analysis.ipynb, main.ipynb)

### ğŸ”„ Repository Philosophy

This repository provides **code and workflows**, not data or pre-trained models. 

**Why?** 
- eBird data files are multi-GB in size
- Models are specific to your dataset
- Following the workflow teaches you the full pipeline
- You maintain control over data quality and preprocessing choices

## ğŸ”§ Development

### Test Notebooks
All test notebooks are passing:
- `test_notebooks/test_vae_module.ipynb` - Tests VAE model architecture
- `test_notebooks/test_data_pipeline.ipynb` - Tests data loading and preprocessing
- `test_notebooks/test_training.ipynb` - Tests training pipeline
- `test_notebooks/test_inference.ipynb` - Tests embedding extraction

### Training Notebooks
- `notebooks/train_vae_kerala.ipynb` - Example VAE training workflow

### Utility Scripts
Run from project root:
```bash
python scripts/train_new_model.py            # Train new VAE model
python scripts/check_model_compatibility.py   # Verify model format
python scripts/test_extractor_fix.py         # Quick inference test
```

### Model Compatibility

âš ï¸ **Important**: Always save models using `save_model_for_inference()` to ensure compatibility with the inference module.

**Correct way:**
```python
from src.training import save_model_for_inference

save_model_for_inference(
    model=vae_model,
    filepath='models/my_model.pth',
    input_dim=467,
    latent_dim=16,
    hidden_dims=[512]
)
```

**Wrong way (will not work with inference module):**
```python
torch.save(vae_model, 'model.pth')  # âŒ Don't do this
```

## ğŸ“ Documentation

- **Main README** (this file): Project overview and quick start
- **Module READMEs**: Detailed docs in each `src/` subdirectory
  - `src/models/README.md` - VAE architecture details
  - `src/data/README.md` - Data pipeline documentation  
  - `src/training/README.md` - Training utilities
  - `src/inference/README.md` - Embedding extraction guide
- **Fix Documentation**: `docs/` folder
  - `INFERENCE_FIX_SUMMARY.md` - Technical details of the inference fix
  - `TEST_INFERENCE_READY.md` - How to use test_inference.ipynb
  - `PHASE_2_SUMMARY.md` - Phase 2 development summary
  - `ORGANIZATION_SUMMARY.md` - Project organization details

## â“ FAQ & Troubleshooting

### Where is the data and models?

**Not included** due to GitHub size limits. Follow these steps:

1. Download eBird data from https://ebird.org/data/download
2. Run the complete workflow in [Getting Started](#-getting-started) to:
   - Create species matrices
   - Train your VAE model
   - Extract embeddings

### What eBird data file do I need?

Request the **eBird Basic Dataset (EBD)** for your region of interest. For Kerala, India:
- Region code: `IN-KL`
- File format: Sampling event data (TSV)
- Example: `ebd_IN-KL_smp_relSep-2025.txt`



### Model is too large / running out of memory?

Reduce model size:
```python
model = VariationalAutoencoder(
    input_dimension=len(species_list),
    latent_dimension=8,      # Reduce from 16
    hidden_dimension=256     # Reduce from 512
)
```

Or filter data more aggressively:
```python
matrix, species = create_species_matrix(
    data,
    min_species_observations=100,  # Increase from 30
    min_checklist_species=10       # Increase from 5
)
```

### How do I choose hyperparameters?

**Start with defaults**, then tune:
- `latent_dimension`: 8-32 (smaller = more compression, larger = more info retained)
- `hidden_dimension`: 256-512 (larger = more capacity)
- `noise_std`: 0.05-0.2 (higher = more regularization)
- `learning_rate`: 1e-4 to 1e-3

### Inference module not loading my model?

Make sure you saved the model correctly:
```python
from src.training import save_model_for_inference

save_model_for_inference(
    model=model,
    filepath='models/my_model.pth',
    input_dim=len(species_list),
    latent_dim=16,
    hidden_dims=[512]
)
```

**Don't use** `torch.save(model, ...)` directly!

## ğŸ“š References

- **Kingma & Welling (2013)**: "Auto-Encoding Variational Bayes" - Original VAE paper
- **Doersch (2016)**: "Tutorial on Variational Autoencoders" - Excellent introduction
- **eBird**: https://ebird.org/ - Bird observation data source

---

## ğŸ—‚ï¸ Example Projects

### District Prediction

Predict Kerala district from bird checklist embeddings using Random Forest classifier.

**Location:** `projects/district_prediction/`

**Goal:** Train a classifier to predict which of 14 Kerala districts a checklist comes from using only the 16-dimensional embedding.

**Key Results:** ~70-80% accuracy on district prediction

**Workflow:**
1. `01_exploration.ipynb` - Load data, extract embeddings, add district labels
2. `02_analysis.ipynb` - Train Random Forest classifier
3. `02b_neural_network.ipynb` - Train neural network classifier
4. `03_results.ipynb` - Visualize confusion matrix, feature importance

---

### Wetland Habitat Classification

Predict whether a checklist location is near wetland habitat using species composition embeddings.

**Location:** `projects/wetland_prediction/`

**Goal:** Determine if VAE embeddings capture habitat information using three labeling approaches:

1. **Bird Proportion Heuristic:** Label based on % of wetland-indicator species (94.6% accuracy)
2. **Hotspot Name Heuristic:** Label based on location name keywords (87.9% accuracy)
3. **Intersection Approach:** Combine both heuristics for stricter labels (97.0% accuracy)

**Key Insight:** High accuracy on hotspot-based labels (independent of species data) proves embeddings learned meaningful habitat associations.

**Workflow:**
1. `00_preprocessing_proportion_heuristic.ipynb` - Create bird-based labels
2. `00b_preprocessing_hotspot_heuristic.ipynb` - Create location-based labels
3. `01_exploration.ipynb` - Load embeddings and labels
4. `02_analysis.ipynb` - Train classifiers and compare approaches

---

**Last Updated**: January 8, 2026  
**Status**: âœ… Core modules complete, tested, and documented  
**Note**: Data and models not included - generate using provided workflows
